"_items": [
    {
      "title": "Twitter image cropper",
      "body": "In September 2020, Twitter's photo cropping algorithm faced criticism for possible racial bias. Users noticed that the algorithm tended to prioritise white faces over black faces when cropping images, leading to different outcomes for the same image with different racial faces. This raised concerns about perpetuating racial biases and impacting users' experiences on the platform.\n\nWatch the video to learn more about this story.\n\n<iframe src=\"https://player.vimeo.com/video/817021580?h=4f28e6368a&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" width=\"100%\" height=\"400\" title=\"twitter\"></iframe>\n\nIn summary, upon investigation, Twitter discovered that their algorithm had\nWith these figures in mind, it was decided that giving people autonomy over the preview and customisation of the crops was the more appropriate outcome.\n\nSharing learnings about our image cropping algorithm - Twitter Blog",
      "_imageAlignment": "full",
      "_graphic": {
        "src": "",
        "alt": "",
        "attribution": "Copyright © 2019"
      },
      "_classes": ""
    },
    {
      "title": "Facebook soap dispenser",
      "body": "In 2017, a Facebook employee in Nigeria shared footage of themselves that speaks to Tech’s larger diversity problem. In the video, a white man and a dark-skinned black man both try to get soap from a soap dispenser. The soap dispenses for the white man, but not the darker skinned man.\n\nThis video of the incident went viral, raising concerns about potential racial bias in technology. The lack of diversity in the training data is pointed out as a possible cause of the biassed outcome, emphasising the importance of using inclusive data sets for AI technology development.\n\nThe incident has sparked discussions on racial discrimination in AI algorithms and has amplified calls for greater awareness and responsibility in developing AI systems to avoid unintentional discrimination. Overall, it serves as a reminder of the need for ethical and diverse testing approaches to ensure fairness in AI technologies.\n\n<iframe src=\"https://player.vimeo.com/video/817030963?h=b9ef979956&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" width=\"100%\" height=\"400\" title=\"why is data ethics important\"></iframe>\n\nWhile soap dispensers are a more light-hearted example, this same problem has been documented in wearable fitness trackers and heart-rate monitors, and also in medical applications.",
      "_imageAlignment": "full",
      "_graphic": {
        "src": "",
        "alt": "",
        "attribution": "Copyright © 2019"
      },
      "_classes": ""
    },
    {
      "title": "Skin cancer diagnosis",
      "body": "Early diagnosis is crucial for positive health results in diseases like skin cancer. For instance, if Melanoma is diagnosed early, an individual has a 100% chance of surviving over 1 year. However, this drops to 53% if diagnosed at a later stage. Notably, people with non-white skin, though less prone to skin cancer, exhibit lower 5-year survival rates (70%) compared to white individuals (92%) [1].\n\nThis highlights issues within our healthcare systems, but also an issue for those organisations who are developing technologies that utilise machine learning to support diagnosis. The video below explores this example in more depth and introduces the First Derm [2] dataset as an example.\n\n<iframe src=\"https://player.vimeo.com/video/817240774?h=1cdc6f2d90&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" width=\"100%\" height=\"400\" title=\"why is data ethics important\"></iframe>\n\nBroadly, there are two proposed approaches to mitigating this bias:\n\nBoth approaches require different considerations for the creation and use of data, and whilst both have merit as approaches to mitigation there are still further considerations to be taken into account to address future challenges.\n\nIt is not just non-white people who are excluded from medical research. In her book “Invisible women” Caroline Criado Perez looks at how much of the world has been designed for men. Among other examples, Caroline reveals how women are 47% more likely to be seriously injured in a car accident due to the seatbelt design.\n\n[1] Gupta, A. K., Bharadwaj, M., & Mehrotra, R. (2016). Skin Cancer Concerns in People of Color: Risk Factors and Prevention. Asian Pacific journal of cancer prevention, 17(12), 5257–5264. Retrieved from https://doi.org/10.22034/APJCP.2016.17.12.5257\n\n[2] Kamulegeya, L., Okello , M., Bwanika, J., Musinguzi, D., Lubega, W., Rusoke, D., & Nassiwa, F. (2019). Using artificial intelligence on dermatology conditions in Uganda: A case for diversity in training data sets for machine learning. bioRxiv",
      "_imageAlignment": "full",
      "_graphic": {
        "src": "",
        "alt": "",
        "attribution": "Copyright © 2019"
      },
      "_classes": ""
    },
    {
      "title": "Criminal profile analysis",
      "body": "Human rights say that everyone is entitled to a fair trial in crime and justice. Rating a risk of future crime is often done in conjunction with an evaluation of rehabilitation needs to ensure that aspects such as jail term and type of prison match the likelihood of reoffense. But is using an algorithm to predict risk of recidivism a good idea?\n\nLet’s explore how data is helping (or hindering) crime and justice:\n\n<iframe src=\"https://player.vimeo.com/video/817271706?h=ff16aca3bd&badge=0&autopause=0&player_id=0&app_id=58479\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" width=\"100%\" height=\"400\" title=\"Data Ethics - Criminal Faces\"></iframe>\n\nIn this example, we explored the cases of Vernon Prater and Brisha Borden. The risk scores applied to these individuals is highly problematic, and as explained, demonstrates how the use of data can lead to remarkably unreliable results.\n\nOverall, Northpointe’s assessment tool (COMPAS) correctly predicts recidivism about 61% of the time, but was much more likely to falsely flag black defendants as future criminals, wrongly labeling them as future criminals at almost twice the rate as white defendants. In contrast, white defendants were mislabeled as low-risk more often than black defendants.\n\nIt has also been used to assign risk scores to offenders in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington and Wisconsin. These risk scores are used to inform decisions about who can be set free at every stage of the criminal justice system, from assigning bond amounts to making parole decisions.\n\nThe two cases in the video demonstrate how AI and machine learning models must be thoroughly tested and evaluated for fairness and impartiality, and how their implementation must be done carefully to avoid unintentional biases. Furthermore, it underscores the need for proper regulation, validation, and oversight to ensure that these models are used in a fair and just manner.",
      "_imageAlignment": "full",
      "_graphic": {
        "src": "",
        "alt": "",
        "attribution": "Copyright © 2019"
      },
      "_classes": ""
    }